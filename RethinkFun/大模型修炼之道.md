### Attention

在 **Attention** 机制中，Q, K, V 是分别代表 **Query**、**Key** 和 **Value** 的向量。这三个向量是 Attention 机制的核心部分，用于计算输入序列中的元素之间的相关性。让我们分别解释它们的含义：

**1. Q（Query，查询向量）：**

**Query** 向量是你当前想要查询的信息。对于输入序列中的某个元素，它会生成一个查询向量，用于表示该元素想要从其他元素中获取的信息。

例如，如果你想找到哪个单词在句子中与当前单词最相关，那么 Query 向量就是用来做这个查询的。

**2. K（Key，键向量）：**

**Key** 向量是与输入序列中每个元素相关联的向量，用于表示这个元素的特征。对于每个输入元素，它会生成一个 Key 向量，代表该元素“拥有”的信息。

**Key** 和 **Query** 的作用是配合使用，来计算 Query 与其他元素的相似度（相关性）。

**3. V（Value，值向量）：**

**Value** 向量是 Attention 机制最终要提取的信息。在 Attention 中，当你找到最相关的 Key 后，Value 就是与这个 Key 对应的实际信息内容。最终，Attention 会根据 Query 与 Key 的相似度，对 Value 进行加权，得到注意力输出。

**Attention 机制的流程：**

**计算相似度**：首先，对于每一个输入元素，将其 Query 与其他元素的 Key 进行点积运算，得到一个相似度分数，用来衡量当前元素（Query）与其他元素（Key）之间的关系强弱。

点积越大，表示 Query 和 Key 之间的相关性越强。

**加权求和**：将相似度分数进行 softmax 归一化，然后对所有元素的 Value 向量进行加权求和，权重由 Query 和 Key 的相似度决定。

**输出结果**：加权求和的结果就是 Attention 机制的输出，它是所有 Value 向量的加权组合，反映了当前 Query 想要从其他元素中提取到的信息。

**举例说明：**

假设我们有一个句子 “The cat sat on the mat”。我们使用 Attention 来计算每个单词与其他单词的关系：

对于单词 “cat” 生成的 Query 向量，用来“询问”它与句子中其他单词的关系。

每个单词（包括 “cat” 本身）都有自己的 Key 向量，表示它们各自的特征。

计算 “cat” 的 Query 与其他单词的 Key 的相似度，得到一个权重分布，然后用这个权重分布来加权求和所有单词的 Value 向量，得到最终的 Attention 输出。

通过 Attention 机制，模型能够关注到句子中与当前单词相关的重要信息，而忽略不相关的部分。

**总结：**

**Q（Query）**：表示你想要查找的信息。

**K（Key）**：表示每个元素携带的特征。

**V（Value）**：表示每个元素实际包含的信息，最终 Attention 的输出依赖于这些值。

### Softmax

公式：
$$
\text{Softmax}(z_i) = \frac{\exp(z_i)}{\sum_{j=1}^{n} \exp(z_j)}
$$
