# 摘要

本文使用 [vLLM][vLLM]（一种流行的 LLM 服务开源库）通过 Ray Serve 运行大型语言模型。[vLLM][vLLM] 使用 OpenAI Chat Completions API，可轻松与其他 LLM 工具集成。该示例还使用放置组通过 Ray Serve 设置多 GPU 或多 HPU 服务。如需更多高级功能（如具有服务多路复用的多 lora 支持、JSON 模式函数调用和进一步的性能改进），请尝试 Anyscale 上的 LLM 部署解决方案。







[vLLM]:https://github.com/vllm-project/vllm







