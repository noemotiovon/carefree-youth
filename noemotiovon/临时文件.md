### 修改前BNSD

==== FusedInferAttention Inputs ====

Q tensor (src0) ne: [64 2 14 1 ]
K tensor (src1) ne: [64 256 2 1 ]
V tensor (src2) ne: [64 256 2 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 2 14 1 ]
Broadcasted mask tensor nb: [2 512 0 1024 ]
numHeads = 14, numKeyValueHeads = 2
layout = BNSD
scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================

==== FusedInferAttention Inputs ====

Q tensor (src0) ne: [64 20 14 1 ]
K tensor (src1) ne: [64 256 2 1 ]
V tensor (src2) ne: [64 256 2 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 20 14 1 ]
Broadcasted mask tensor nb: [2 512 0 10240 ]
numHeads = 14, numKeyValueHeads = 2
layout = BNSD
scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================

==== FusedInferAttention Inputs ====

Q tensor (src0) ne: [64 1 14 1 ]
K tensor (src1) ne: [64 256 2 1 ]
V tensor (src2) ne: [64 256 2 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 1 14 1 ]
Broadcasted mask tensor nb: [2 512 0 512 ]
numHeads = 14, numKeyValueHeads = 2
layout = BNSD
scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================

### 修改后BSND

==== FusedInferAttention Inputs ====

Q tensor (src0) ne: [64 14 2 1 ]
K tensor (src1) ne: [64 2 256 1 ]
V tensor (src2) ne: [64 2 256 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 2 14 1 ]
Broadcasted mask tensor nb: [2 512 0 1024 ]
numHeads = 14, numKeyValueHeads = 2
layout = BSND
scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================

==== FusedInferAttention Inputs ====

Q tensor (src0) ne: [64 14 20 1 ]
K tensor (src1) ne: [64 2 256 1 ]
V tensor (src2) ne: [64 2 256 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 20 14 1 ]
Broadcasted mask tensor nb: [2 512 0 10240 ]
numHeads = 14, numKeyValueHeads = 2
layout = BSND
scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================

==== FusedInferAttention Inputs ====
Q tensor (src0) ne: [64 14 1 1 ]
K tensor (src1) ne: [64 2 256 1 ]
V tensor (src2) ne: [64 2 256 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 1 14 1 ]
Broadcasted mask tensor nb: [2 512 0 512 ]
numHeads = 14, numKeyValueHeads = 2
layout = BSND

scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================

Decode（BSND）

==== FusedInferAttention Inputs ==== 

Q tensor (src0) ne: [64 1 14 1 ]
K tensor (src1) ne: [64 256 2 1 ]
V tensor (src2) ne: [64 256 2 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 14 1 1 ]
Broadcasted mask tensor nb: [2 512 0 7168 ]
numHeads = 1, numKeyValueHeads = 256
layout = BSND
scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================

==== FusedInferAttention Inputs ====
Q tensor (src0) ne: [64 1 14 1 ]
K tensor (src1) ne: [64 256 2 1 ]
V tensor (src2) ne: [64 256 2 1 ]
Mask tensor (src3) ne: [256 64 1 1 ]
Broadcasted mask tensor ne: [256 14 1 1 ]
Broadcasted mask tensor nb: [2 512 0 7168 ]
numHeads = 1, numKeyValueHeads = 256
layout = BSND

scaleValue = 0.125000, maxBias = 0.000000, logitSoftcap = 0.000000

===================================