### 1 知识导入

#### 1. 1 c/c++文件执行过程

**1. 编写代码（Source Code）**

**工具**：文本编辑器（如vim、VS Code、Sublime Text等）。

**操作**：编写C/C++源代码文件，通常扩展名为.c或.cpp。

**2. 编译（Compilation）**

**工具**：C/C++编译器（如gcc、g++、clang）。

**命令**：在命令行中可以使用以下命令将源码编译成汇编代码（可选步骤，也可以直接生成目标文件）：

```bash
gcc -S source.c -o source.s   # C语言
g++ -S source.cpp -o source.s # C++语言
```

**说明**：编译器将源码转换为汇编代码（可读的低级代码），包括语法检查、语义检查、优化等过程。

**3. 汇编（Assembly）**

**工具**：汇编器（如as）。

**命令**：可以将汇编代码转换为二进制的目标文件（.o文件）：

```bash
gcc -c source.s -o source.o
```

（大多数情况下直接使用-c选项编译成目标文件，跳过生成汇编代码的步骤。）

**说明**：汇编器将汇编代码转换为二进制的机器代码（目标文件），这是特定于硬件架构的文件。

**4. 链接（Linking）**

**工具**：链接器（通常由编译器自动调用，例如GCC中的ld）。

**命令**：将多个目标文件链接成一个可执行文件：

```bash
gcc source.o -o executable   # C语言
g++ source.o -o executable   # C++语言
```

**说明**：链接器将目标文件和库文件（如C标准库、动态库）整合到一起，生成最终的可执行文件。链接过程包括符号解析和地址分配。

**5. 使用CMake进行构建（可选）**

对于大型项目或多文件项目，通常使用构建系统来简化编译和链接过程。CMake是一个常见的构建工具，它会生成Makefile或其他平台的项目文件。

**工具**：CMake

**命令**：

```bash
# 生成构建文件
cmake -B build -DCMAKE_BUILD_TYPE=Release

# 构建项目
cmake --build build
```

**说明**：CMake会自动生成构建脚本，并调用相应的编译器和链接器，简化了项目的管理和构建过程。

**6. 运行可执行文件**

**工具**：操作系统

**命令**：

```bash
./executable  # 在Linux/macOS
executable.exe # 在Windows
```

**说明**：通过执行生成的文件来运行程序。



### 2 llama.cpp 项目运行

#### 2.1 下载llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

#### 2.2 编译

```
# CPU，llama.cpp在根目录运行命令
make

# GPU，llama.cpp在根目录运行命令
make LLAMA_CUDA=1

# NPU，llama.cpp在根目录运行命令
cmake -B build -DGGML_CANN=on -DCMAKE_BUILD_TYPE=release
cmake --build build --config release
```

#### 2.3 模型格式转换

**创建conda虚拟环境**

```bash
conda create -n llama.cpp python==3.10

# llama.cpp在根目录运行命令
pip install -r requirements.txt

# 激活环境
conda activate llama.cpp
```

**转换（根据模型架构，可以使用`convert.py`或`convert-hf-to-gguf.py`文件）**

```bash
# 在llama.cpp根目录下
python3 convert_hf_to_gguf.py   /home/lcg/.cache/modelscope/hub/Qwen/Qwen2-0___5B-Instruct/ --outfile /home/lcg/gguf_model/Qwen2-0___5B-Instruct.gguf
```

#### 2.4 验证设备正确

```bash
./build/bin/llama-cli -m PATH_TO_MODEL -p "Building a website can be done in 10 steps:" -ngl 32
```

If the fllowing info is output on screen, you are using `llama.cpp by CANN backend`:

```
llm_load_tensors:       CANN0 buffer size = 13313.00 MiB
llama_new_context_with_model:       CANN0 compute buffer size =  1260.81 MiB
```

