* python

  ```python
  def __init__()
  ```

  类似于java的构造方法

  ```python
  def __str__()
  ```

  类似于java的toString()

  ```python
  return f"{self.name} is {self.age} years old."
  ```

  Python 中的格式化字符串（f-string）语法

  ---

* python中，文件名前加一个_是什么意思？

  **1. 私有模块**：在 Python 中，以单个下划线开头的模块（或文件）通常意味着它是供内部使用的，不建议从外部直接导入或访问。例如，_my_private_module.py。

  **2. 命名约定**：这不是强制性的规则，而是一种约定，用于提醒开发者该模块或文件是私有的。Python 语言本身并没有像一些其他语言（如 Java 或 C++）那样强制实施私有性。

  **3. 避免冲突**：使用下划线前缀的模块可以减少与其他模块的命名冲突，尤其是在大型项目中，帮助维护代码的可读性和结构。

  ---

* python文件名命名`__init__`是做什么用的？

  **1. 标识包**：任何包含 __init__.py 文件的目录都被视为一个 Python 包。这使得 Python 能够将该目录及其子目录视为一个模块集合，可以被导入和使用。

  **2. 初始化代码**：__init__.py 文件可以包含包的初始化代码。当包被导入时，__init__.py 文件中的代码会自动执行。这可以用于设置包级别的变量、导入子模块或定义包的接口。

  **3. 控制导入内容**：通过在 __init__.py 中定义 __all__ 变量，可以控制当使用 from package import * 语句时，哪些模块或对象会被导入。例如：

  ```python
  # __init__.py
  __all__ = ['module1', 'module2']
  ```

  **4. 空文件**：在某些情况下，__init__.py 可以是一个空文件，但它的存在依然是为了确保 Python 将目录识别为包。

  ---

* return _component_(*args, **kwargs) 前面的*和**是什么意思？

  ***args**:

  这个语法用于接收任意数量的位置参数，参数会以元组的形式传递给函数。例如，如果函数定义为 def func(*args):，那么在调用 func(1, 2, 3) 时，args 将会是 (1, 2, 3)。

  在你提到的代码中，_component_(*args, **kwargs) 将传递所有位置参数给 _component_。

  ***\*kwargs**:

  这个语法用于接收任意数量的关键字参数，参数会以字典的形式传递给函数。例如，定义为 def func(**kwargs): 的函数可以接受类似 func(a=1, b=2) 的调用，其中 kwargs 将会是 {'a': 1, 'b': 2}。

  在代码中，_component_(*args, **kwargs) 也将所有关键字参数传递给 _component_。

  ---

* ```python
  def left_pad_sequence(
      sequences: List[torch.Tensor],
      batch_first: bool = False,
      padding_value: float = 0,
  ) -> torch.Tensor:
  ```

  left_pad_sequence 是一个用于对变长张量序列进行左侧填充的函数。它接收一个张量列表 sequences，一个布尔值 batch_first（用于指示输出格式），以及一个填充值 padding_value（默认为0）。

  ---

* ```python
  class ABC(metaclass=ABCMeta): 
  ```

  **1 ABC** **(Abstract Base Class)**:

  ABC 是一个抽象类的名称，它的作用是作为其他类的基类，不能直接实例化。抽象类通常包含一个或多个抽象方法，这些方法必须在子类中实现。

  当你想创建一个不能直接使用的类（但希望其他类从中继承并实现具体功能）时，你就会定义一个抽象基类。

  **2 metaclass=ABCMeta**:

  ABCMeta 是 Python 标准库中的一个元类，它用于定义抽象类和抽象方法。

  当你将 metaclass 设置为 ABCMeta 时，它会使类成为抽象基类，并允许其中定义抽象方法。

  抽象方法是使用 @abstractmethod 装饰器定义的。这些方法不能在基类中实现，必须由子类来实现。

  **3. 元类的作用**:

  在 Python 中，元类用于控制类的创建行为。ABCMeta 通过继承自 type，增强了类的创建流程，使其能够识别和处理抽象方法。

  通过定义抽象基类，你可以强制子类必须实现特定的方法，从而保证接口的一致性。这种方法常用于设计模式中的模板模式。

  ---

* if `__ name __`== `"__main__"`: 是 Python 程序中常用的一个代码结构，目的是确保某些代码块只有在脚本作为主程序运行时才会被执行，而当脚本被作为模块导入时则不会执行这些代码。

  **代码解释：**

  ```python
  if __name__ == "__main__":
      sys.exit(recipe_main())
  ```

  **使用场景：**

  这段代码典型用于控制程序的执行流程。开发者可以将主要逻辑封装在 recipe_main() 中，确保该逻辑只有在脚本作为主程序运行时才会执行，而当该模块被导入时不会执行这部分代码，从而避免不必要的运行行为。

  **示例：**

  ```python
  # 假设文件名为 example.py
  
  def recipe_main():
      print("This is the main function.")
      return 0  # 表示正常退出
  
  if __name__ == "__main__":
      sys.exit(recipe_main())
  ```

  如果直接运行 example.py，它会输出 This is the main function.，然后程序正常退出，返回状态码 0。

  如果从另一个 Python 文件中导入 example.py，recipe_main() 就不会自动运行。

  ---

* **装饰器 @config.parse**

  @config.parse 是一个装饰器，作用是对函数 recipe_main 的参数进行预处理。它将配置文件或命令行参数解析成一个结构化的 cfg（通常是 DictConfig 类型，来自 OmegaConf 库）。

  这个装饰器的主要功能是处理用户的配置文件或者命令行参数，使得它们以一种统一的格式传递给 recipe_main 函数。

  **OmegaConf** 是一种用于处理嵌套字典、支持层次化配置的 Python 库，经常在机器学习项目中用于管理复杂的配置。

  ---

* torch.optim 是 PyTorch 中的一个模块，用于实现各种优化算法，这些算法用于更新模型参数以最小化损失函数。它为用户提供了多种常用的优化器，简化了训练过程中的参数更新步骤。以下是一些主要功能和特点：

  **主要功能**

  * **实现多种优化算法**：torch.optim 包含多种常用的优化算法，如：
    * **SGD (Stochastic Gradient Descent)**：随机梯度下降，支持动量和学习率调度。
    * **Adam**：自适应动量估计优化器，结合了 RMSprop 和动量的优点。
    * **RMSprop**：对每个参数使用不同的学习率。
    * **Adagrad**、**Adadelta**、**AdamW**等：这些都是不同的自适应学习率算法。
  * **参数更新**：优化器的主要功能是根据计算出的梯度更新模型的参数。用户需要在反向传播后调用优化器的 step() 方法来执行参数更新。
  * **学习率调度**：torch.optim 还可以与学习率调度器（如 torch.optim.lr_scheduler）结合使用，以动态调整学习率，从而提高训练效果。

  ---

* HuggingFace设置国内镜像：

  ```bash
  pip install -U huggingface_hub
  ```

  ```bash
  export HF_ENDPOINT=https://hf-mirror.com
  ```

* `cannot allocate memory in static TLS block` 报错解决

  当你遇到“cannot allocate memory in static TLS block”这个错误时，通常意味着程序在尝试初始化线程局部存储（TLS, Thread Local Storage）时遇到了内存分配问题。TLS允许每个线程拥有自己的变量副本，这对于多线程程序是非常有用的。
  ```bash
  # 文件地址替换为报错的文件地址
  export LD_PRELOAD=/home/lcg/miniconda3/envs/opencompass/lib/python3.10/site-packages/sklearn/utils/../../scikit_learn.libs/libgomp-d22c30c5.so.1.0.0
  ```

* git将多个commits合并成一个

  1. 找到远程端最后一个commit，右键并选择复制commit id。

  2. ```bash
     git reset --soft 33b8143d9d2b01cd5b6fe97091f2987f081146b6
     ```

  3. ```bash
     git commit -m "[HardWare]Add NPU Support"
     ```

  4. ```
     git push --force origin npu_support
     ```

* **Softmax 公式**

  对于输入向量$ z = [z_1, z_2, \ldots, z_n]$，softmax 函数定义为：
  $$
  \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
  $$
  其中， e^{z_i} 表示以自然数 e 为底的指数运算，分母是所有 e^{z_j} 的和，确保输出的和为 1。

* ReLU（Rectified Linear Unit）函数是一种在深度学习中非常常见的激活函数，其作用是将输入值中的负数截断为零，而保留正数不变。ReLU 函数的定义非常简单：
  $$
  \text{ReLU}(x) = \max(0, x)
  $$

* Sigmoid 函数是一种常用的激活函数，尤其是在二分类问题和神经网络中。它将输入值映射到 0 和 1 之间，形成一个 S 形的曲线。Sigmoid 函数的数学表达式为：
  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}
  $$

* 真实的梯度下降是对同一批数据同时执行的，这样可以保障避免单个数据上的规律与其他数据规律过于冲突，因此正确的全称是随机批梯度下降，所以越是复杂的数据，学习率越不能设置太大，否则容易造成在新一批数据上损失无法下降，因此如何让损失稳定下降，是个练丹级别的经验艺术，需要对数据和模型都有非常深刻的理解。

* 学习率是优化算法中的一个调整参数，它决定了每次迭代的步骤大小，同时向损失函数的最小值移动。由于它影响到新货的的信息在多大程度上覆盖了旧信息，它比喻为机器学习模型的学习速度。

* 算力单位OPS( Operations Per Second), 表示处理器每秒进行多少次计算。

* MACs(Multiply-Accumulate Operations), 矩阵乘加累计计算。

* FLOPs(Floating Point Operations), 浮点运算次数

* AI计算关键指标

  * 精度 Accuracy
  * 吞吐量 Throughput
  * 时延 Latency
  * 能耗 Energy
  * 系统价格 System Cost
  * 易用性 Flexibility

* **流式接口**（Fluent Interface）是一种编程风格，旨在通过链式调用让代码看起来更直观和简洁。这种接口通常将每个方法的返回值设置为对象本身，允许多个方法连续调用在同一行代码中，形成类似“流”一样的调用链。

* 张量的**重复广播（broadcasting）** 是指将一个较小形状的张量通过某种规则扩展，使其能够与较大形状的张量进行逐元素操作（如加法、乘法等）。这在机器学习、深度学习中非常常见，因为它允许我们用更简单的方式处理不同形状的张量。

  1. **从右到左匹配张量的维度（shape）**。较小张量的维度要么与较大张量的对应维度相同，要么是 1（可以扩展为匹配的维度）。

  2. 如果较小张量在某个维度为 1，则该维度会被**重复**（重复的次数是较大张量对应维度的大小）。

  3. 如果较小张量在某个维度不存在（即较小张量的维度更少），可以通过在最左侧补充 1 来进行对齐。
  4. **可以重复广播的张量可以广播后进行加法乘法运算操作。**

  **Example**：

  ```
  src0.shape -> (4,3,2) | src1.shape -> (3,2) 可以重复广播
  src0.shape -> (4,3,2) | src1.shape -> (1,2) 可以重复广播
  src0.shape -> (4,3,2) | src1.shape -> (2,2) 不可以重复广播
  src0.shape -> (4,3,2) | src1.shape -> (1,3,2) 可以重复广播
  ```

* 在分布式深度学习中，为了加速训练和处理超大模型，通常会使用不同的并行策略。数据并行、Pipeline并行、张量并行、和模型并行是四种主要的并行方式。以下是它们的区别和应用场景：

  1. 数据并行 (Data Parallelism)
    概念： 数据并行是指将完整的模型副本分布到多个计算设备（如多个GPU）上，将训练数据分成小批次，每个设备处理不同的批次数据，计算梯度。计算完梯度后，所有设备的梯度会汇总（通常通过求和或平均），用于更新模型参数。

    应用场景：

    适用于模型相对较小但数据量非常大的场景。
    深度学习框架支持良好，易于实现。
    优缺点：

    优点： 实现简单，扩展性强。
    缺点： 对于超大模型，模型本身可能无法放入单个设备的内存。

  2. Pipeline并行 (Pipeline Parallelism)
    概念： 在Pipeline并行中，模型的不同层或模块被划分到不同的设备上处理。输入数据被分成小批次（Micro-batch），数据流通过各层时，每个设备只处理模型的一部分，形成流水线。这样当一个设备在处理当前数据时，下一个设备可以处理前一个批次的数据。

    应用场景：

    适用于超大模型，尤其是层次结构较深的模型。
    需要模型的各个部分可以独立计算，以最大化流水线的效率。
    优缺点：

    优点： 允许处理超大模型，解决内存限制问题。
    缺点： 实现复杂，可能出现设备间同步问题，延迟较大。

  3. 张量并行 (Tensor Parallelism)
    概念： 张量并行是指将模型的单个张量（如权重矩阵、输入张量）切分成多个部分，并分配到不同的设备上进行并行计算。例如，将矩阵分割到多个设备上，分别计算一部分，再将结果组合。

    应用场景：

    适用于需要进一步并行化的超大模型场景。
    通常与其他并行方法（如数据并行、Pipeline并行）结合使用。
    优缺点：

    优点： 适合处理非常大的张量，允许更细粒度的并行计算。
    缺点： 实现复杂，需要高效的设备间通信。

  4. 模型并行 (Model Parallelism)
    概念： 模型并行是指将整个模型按层或模块划分到不同的设备上，每个设备只存储和计算模型的一部分。与Pipeline并行不同，模型并行中的各部分可能同时处理同一批次的数据。

    应用场景：

    适用于模型过大，无法在单个设备内存中完全加载的情况。
    适合模型结构较松散且各部分计算需求均衡的模型。

    优缺点：

    优点： 解决超大模型的内存限制问题。
    缺点： 如果模型部分间的依赖性强，可能会导致设备间通信瓶颈，增加同步复杂性。

  5. 总结

    数据并行：同一模型副本在不同设备上处理不同的数据子集，适合数据量大但模型较小的场景。
    Pipeline并行：模型按层分配到不同设备上，以流水线方式处理数据，适合超大模型。
    张量并行：单个张量被切分到多个设备上并行计算，适合超大张量的细粒度并行。
    模型并行：模型按模块或层分配到不同设备上，每个设备处理模型的一部分，适合超大模型。


​	引用：[CSDN](https://blog.csdn.net/yxx122345/article/details/141716708)















