### 预训练和微调的区别

预训练的目标、采用的数据集、需要的GPU数量都不同。

预训练采用随机初始化模型参数的方式，构造模型，然后通过大量的未打标签数据进行训练，学习语料的通用特征；而微调会从预训练模型中加载参数，保留了预训练过程中学到的通用特征，通过少量高质量的打标签语料来训练模型，以提高模型在特定任务上的能力和性能。

上文提到的参数包含：权重,偏置,Word Embeddings,Positional Encoding,注意力机制参数等。

**预训练（Pre-Training）**

 **预训练**的主要目标是通过大规模的无监督数据集（如文本语料库）来学习语言的基本结构和语义特征。预训练通常涉及以下步骤：

1. **随机初始化权重**：模型的参数，如权重和偏置在预训练开始时是随机初始化的。
2. **大规模数据集**：使用大量的无监督数据进行训练。
3. **学习通用特征**：模型通过优化损失函数（如语言模型的交叉熵损失）来学习语言的通用特征。

预训练的关键点：

- **随机初始化**：模型的所有参数（权重、偏置等）在预训练开始时是随机的。
- **大规模数据**：使用大规模的无监督数据集进行训练。
- **通用特征**：学习语言的基本结构和语义特征，为后续任务提供一个良好的起点。

**微调（Fine-Tuning）**

**微调**的主要目标是通过特定任务的数据集来优化模型在该任务上的性能。微调通常涉及以下步骤：

1. **加载预训练权重**：模型的权重和偏置从预训练模型中加载。
2. **特定任务数据**：使用特定任务的数据集进行训练。
3. **优化特定任务性能**：模型通过优化损失函数来调整参数，以提高在特定任务上的性能。

微调的关键点

- **加载预训练权重**：模型的参数从预训练模型中加载，保留了预训练过程中学到的通用特征。
- **特定任务数据**：使用特定任务的数据集进行训练。
- **任务优化**：进一步调整模型参数，以优化在特定任务上的性能。

**总结：**

1. **训练效率**：预训练通常需要大量的计算资源和时间，因为它需要在大规模数据集上训练模型的所有参数。而微调则相对高效，因为它在预训练模型的基础上进行，只需要进一步优化特定任务的数据。
2. **模型性能**：预训练模型已经学习了语言的通用特征，这使得微调能够更快地收敛，并且在特定任务上表现更好。直接从随机初始化开始训练特定任务模型，通常需要更多的数据和时间，且性能可能不如预训练+微调的方式。
3. **应用场景**：预训练模型可以作为通用的基础模型，适用于多种下游任务。通过微调，可以快速适应不同的任务需求，而不需要从头开始训练模型。



### 大模型微调步骤

1. **数据准备 (Data Preparation)**

- **步骤**：根据任务需求，收集、清理并格式化数据。
- **技术**：数据增强、数据清洗、数据标注等。
- **目标**：确保数据质量和多样性，以提高模型的泛化能力。

2. **初始化模型 (Model Initialization)**

- **步骤**：加载预训练模型（通常是大模型）及其参数。
- **技术**：预训练模型加载（如BERT、GPT-3等），检查点恢复。
- **目标**：利用预训练权重加速模型收敛，同时保留原始模型的知识。

3. **冻结部分层 (Layer Freezing)**

- **步骤**：根据需求冻结某些层，以减少计算开销或避免模型过拟合。
- **技术**：梯度冻结、层级冻结。
- **目标**：保持预训练模型的一些特征层不变，专注于特定任务的优化。

4. **选择微调方法 (Fine-tuning Method Selection)**

- **步骤**：选择适合任务的微调方式。
- 技术：
  - **全参数微调**：对整个模型进行微调，适合对模型进行深度优化。
  - **部分参数微调**：如只微调顶层参数或特定任务相关的层，适合小数据集任务。
  - **参数高效微调 (PEFT)**：使用如LORA、Prefix Tuning、Adapter等方法，减少计算量和内存占用。
- **目标**：在不同资源条件下找到最适合的微调方式。

5. **学习率调度 (Learning Rate Scheduling)**

- **步骤**：设定微调过程的学习率和调度策略。
- **技术**：线性衰减、余弦退火、预热等调度策略。
- **目标**：控制学习率随训练进展而动态调整，防止模型过拟合或训练不稳定。

6. **正则化 (Regularization)**

- **步骤**：在训练过程中使用正则化技术，防止模型过拟合。
- **技术**：L2正则化、Dropout、早停法（Early Stopping）等。
- **目标**：通过引入噪声或约束防止模型记住训练集，提升泛化能力。

7. **梯度剪裁 (Gradient Clipping)**

- **步骤**：控制梯度更新的大小。
- **技术**：基于最大范数的梯度裁剪。
- **目标**：防止梯度爆炸，稳定训练过程。

8. **模型评估与监控 (Evaluation and Monitoring)**

- **步骤**：在验证集或测试集上评估模型性能，监控各项指标。
- **技术**：准确率、F1、精确率、召回率等评估指标，TensorBoard等监控工具。
- **目标**：确保微调过程中模型的性能符合预期，及时发现问题。

9. **模型验证与超参数优化 (Model Validation and Hyperparameter Tuning)**

- **步骤**：调整超参数以获得最佳模型效果。
- **技术**：网格搜索、贝叶斯优化等。
- **目标**：找到最佳的学习率、批次大小等参数组合。

10. **部署与推理优化 (Deployment and Inference Optimization)**

- **步骤**：将模型部署到生产环境，并进行推理优化。
- **技术**：量化（Quantization）、剪枝（Pruning）、模型蒸馏（Distillation）等。
- **目标**：优化模型推理速度和内存占用，使其在实际应用中高效运行。

### 

### TorchTune 支持的微调技术

![image-20241101114202467](/Users/lichenguang/Library/Application Support/typora-user-images/image-20241101114202467.png)

1. **Full Finetuning**

- **描述**：对整个模型的所有参数进行微调。适用于需要对新任务充分学习的大模型，但计算和存储开销较大。
- **优点**：可以达到最佳的微调效果，适应性强。
- **缺点**：计算资源需求大，特别是对于大模型，训练速度慢且显存占用高。

2. **LoRA Finetuning**

- **描述**：使用低秩矩阵分解，只对少数层的低秩矩阵进行微调，而不是整个权重矩阵，减少参数的更新。
- **优点**：显著减少了训练参数量，资源消耗小。
- **缺点**：无法获得全模型微调的性能提升，适用于参数更新较少的情况。

3. **QLoRA Finetuning**

- **描述**：在LoRA的基础上结合量化技术，使得激活层和LoRA层量化到4-bit表示。大大降低了存储开销。
- **优点**：与LoRA相似，但进一步减少了内存占用，适合极端内存受限环境。
- **缺点**：在量化过程中会引入一些近似误差，可能影响模型精度。

4. **DoRA/QDoRA Finetuning**

- **描述**：
- **优点**：能够在多个设备间并行更新LoRA参数，进一步节省内存和计算。
- **缺点**：分布式训练的复杂性增加。

5. **Quantization-Aware Training (QAT)**

- **描述**：在训练过程中考虑量化操作（如8-bit量化），用于减小模型大小、加快推理速度。
- **优点**：适合部署到内存和计算能力受限的设备上。
- **缺点**：模型精度可能有所下降，训练复杂性增加。

6. **Direct Preference Optimization (DPO)**

- **描述**：通过直接优化偏好函数进行微调，常用于生成任务，使模型输出更贴合用户偏好。
- **优点**：可以通过明确的偏好优化，提高生成内容的质量。
- **缺点**：只适用于有偏好的生成任务，且需要大量偏好数据。

7. **Proximal Policy Optimization (PPO)**

- **描述**：一种强化学习算法，通过优化策略来微调模型，适用于需要策略调整的任务。
- **优点**：具有较好的稳定性，在策略微调中效果较佳。
- **缺点**：训练开销大，且依赖于奖励函数设计。

8. **Knowledge Distillation**

- **描述**：通过教师模型（大模型）指导学生模型（小模型）学习，从而获得较好的压缩效果。
- **优点**：能够显著减小模型体积，同时保持较好的性能。
- **缺点**：依赖于教师模型的效果，且学生模型效果无法完全达到教师模型水平。

微调技术对比表：

| 技术                                 | 描述                     | 优点                         | 缺点           | 适用场景             |
| ------------------------------------ | ------------------------ | ---------------------------- | -------------- | -------------------- |
| Full Finetuning                      | 全参数微调整个模型       | 精度高，适应性强             | 计算资源需求大 | 需要全面微调的任务   |
| LoRA Finetuning                      | 低秩分解，仅微调部分参数 | 参数少，资源消耗小           | 可能精度不足   | 资源有限，适合小改动 |
| QLoRA Finetuning                     | LoRA+量化4-bit           | 内存占用低，适合极端内存限制 | 精度有一定下降 | 内存极端受限场景     |
| DoRA/QDoRA Finetuning                | 分布式LoRA或LoRA+量化    | 低内存、低计算资源消耗       | 实现复杂       | 分布式内存有限环境   |
| Quantization-Aware Training (QAT)    | 训练时考虑量化           | 小模型，适合低计算需求       | 可能损失精度   | 部署在受限设备上     |
| Direct Preference Optimization (DPO) | 优化偏好函数             | 增强用户偏好生成效果         | 需偏好数据     | 有明确偏好需求任务   |
| Proximal Policy Optimization (PPO)   | 强化学习策略优化         | 稳定性好，策略微调           | 高训练开销     | 策略微调和控制任务   |
| Knowledge Distillation               | 大模型指导小模型学习     | 高压缩率，保持性能           | 依赖教师模型   | 需要压缩模型的任务   |



### 大模型微调模型更新

在大模型微调中，通常微调后的参数不会直接修改原模型文件，而是存储在一个**单独的文件**中。这种方法可以节省存储空间并且更灵活。在使用时，微调的参数会在推理阶段加载进原模型中，使原模型能够正确应用这些新参数。具体方法有以下几种：

**1. 直接微调全模型参数**

在少数情况下，模型在微调时会直接更新全模型的权重参数。

这种方法会生成一个完整的新模型文件，但文件较大。

**生效方式**：推理时直接使用微调后的完整模型文件，不再依赖原始模型文件。

**2. 参数高效微调（PEFT）**

**代表方法**：LoRA（低秩适应）、Adapter、Prompt Tuning等。

**做法**：只更新模型的部分参数（例如特定层的低秩矩阵或适配层参数），而不是所有参数。

**参数存储**：这些微调参数会单独保存成新的文件，通常非常小，只记录了需要更新的参数和权重。

**生效方式**：推理时加载原始模型，然后将这些微调参数（如LoRA权重或Adapter层）叠加或合并到原始模型的对应位置，进而使得微调参数生效。

**3. 冻结原始参数，添加微调模块**

**做法**：在不修改原始模型参数的情况下，添加一层或多层新参数模块（如Adapter或额外的前馈层），这些新模块负责微调。

**参数存储**：新模块的参数会存储在单独的文件中。

**生效方式**：推理时加载原模型并添加新模块，使得新的层与原模型层共同作用，最终实现微调效果。

**总结**

在大模型微调中，常见方法是**将微调后的参数保存在额外文件**中，推理时与原始模型配合使用。这样可以高效利用存储资源，避免原模型文件的频繁修改。



