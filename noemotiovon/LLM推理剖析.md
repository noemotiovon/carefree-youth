# 0 引言

本文以 Nano-GPT 模型推理为例，阐述 LLM 推理中各个层之前的相互作用。作为一个缩小版的GPT模型（只拥有85,000个参数），主要目的是为了研究和教育用途。尽管这个模型比OpenAI的GPT-3或GPT-4小得多，但可谓是“麻雀虽小，五脏俱全”。

Nano-GPT的GitHub地址是：https://github.com/karpathy/nanoGPT