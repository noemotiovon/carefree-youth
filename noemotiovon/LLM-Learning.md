###  图形处理器GPU

#### 1 知识预备

- 带宽：处理器能够处理的最大的数据量或指令数量，单位是 Kb、Mb、Gb；
- 延时：处理器执行指令或处理数据所需的时间，传送一个数据单元所需要的时间，单位是 ms、s、min、h 等；
- 吞吐：处理器在一定时间内从一个位置移动到另一个位置的数据量，单位是 bps（每秒比特数）、Mbps（每秒兆比特数）、Gbps（每秒千比特数），比如在第 10s 传输了 20 bit 数据，因此在 t=10 时刻的吞吐量为 20 bps。
- 解决带宽相比较解决延时更容易，线程的数量与吞吐量成正比，吞吐量几乎等于带宽时说明信道使用率很高，处理器系统设计所追求的目标是提高带宽的前提下，尽可能掩盖传送延时，组成一个可实现的处理器系统。



### 2 GPU缓存机制

在 GPU 工作过程中希望尽可能的去减少内存的时延、内存的搬运、还有内存的带宽等一系列内存相关的问题，其中缓存对于内存尤为重要。英伟达 Ampere A100 内存结构中 HBM Memory 的大小是 80G，也就是 A100 的显存大小是 80G。

其中寄存器（Register）文件也可以视为缓存，寄存器靠近 SM（Streaming Multiprocessors）执行单元，从而可以快速地获取执行单元中的数据，同时也方便读取 L1 Cache 缓存中的数据。此外 L2 Cache 更靠近 HBM Memory，这样方便 GPU 把大量的数据直接搬运到 cache 中，因此为了同时实现上面两个目标，GPU 设计了多级缓存。80G 的显存是一个高带宽的内存，L2 Cache 大小为 40M，所有 SM 共享同一个 L2 Cache，L1 Cache 大小为 192kB，每个 SM 拥有自己独立的 Cache，同样每个 SM 拥有自己独立的 Register，每个寄存器大小为 256 kB，因为总共有 108 个 SM 流处理器，因此寄存器总共的大小是 27MB，L1 Cache 总共的大小是 20 MB。

![image-20241030172321661](/Users/lichenguang/Library/Application Support/typora-user-images/image-20241030172321661.png)









### Flash Attention

#### 1 计算限制与内存限制

- π ：**硬件算力上限**。指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 FLOPS or FLOP/s。
- β ：**硬件带宽上限**。指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是Byte/s。
- πt ：**某个算法所需的总运算量**，单位是FLOPs。下标 t 表示total。
- βt ：**某个算法所需的总数据读取存储量，**单位是Byte。下标 t 表示total。

 ：**硬件算力上限**。指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 FLOPS or FLOP/s。

这里再强调一下对FLOPS和FLOPs的解释：

- FLOPS：等同于FLOP/s，表示Floating Point Operations Per Second，即每秒执行的浮点数操作次数，用于衡量硬件计算性能。
- FLOPs：表示Floating Point Operations，表示某个算法的总计算量（即总浮点运算次数），用于衡量一个算法的复杂度。

####  2 kernel融合

举例来说，我现在要做计算A和计算B。在老方法里，我做完A后得到一个中间结果，写回显存，然后再从显存中把这个结果加载到SRAM，做计算B。但是现在我发现SRAM完全有能力存下我的中间结果，那我就可以把A和B放在一起做了，这样就能节省很多读取时间，我们管这样的操作叫**kernel融合**。

**kernel融合和尽可能利用起SRAM，以减少数据读取时间，都是flash attention的重要优化点。**在后文对伪代码的解读中我们会看到，分块之后flash attention将矩阵乘法、mask、softmax、dropout操作合并成一个kernel，做到了只读一次和只写回一次，节省了数据读取时间。