# 强化学习理论导入

## 摘要





## 缩写映射

| 缩写      | 中文描述                | 全称                                       |
| --------- | ----------------------- | ------------------------------------------ |
| **RL**    | **强化学习**            | Reinforcement Learning                     |
| **SFT**   | **监督式微调**          | Supervised Fine-Tuning                     |
| **MDP**   | **马尔可夫决策过程**    |                                            |
| **RLHF**  | **人类反馈强化学习**    | Reinforcement Learning from Human Feedback |
| **RLAIF** | **RLHF 与来自 AI 反馈** |                                            |
| **RM**    | **奖励模型**            | Reward Model                               |
| **PPO**   | **近端策略优化**        | Proximal Policy Optimization               |
|           |                         |                                            |
|           |                         |                                            |

## 强化学习简介

### SFT

SFT 是对 **预训练大语言模型（LLMs）** 进行进一步训练的一种方法，旨在通过给模型提供一系列 **（指令，答案）对** 来增强其在特定任务或领域中的表现。

其中：

- **指令（Instruction）**：是用户输入或任务描述，指导模型该做什么；
- **答案（Answer）**：是人类提供的理想输出，用来教会模型该如何回应。

SFT 的目标是使大语言模型生成更符合特定用途或人类预期的响应，提升模型在实际应用中的实用性和互动性。

**SFT 的优势**

- 能快速将通用模型适配到特定任务或领域。
- 输出更具可控性、准确性，特别是在高风险或专业场景中。
- 训练方法相对直接，技术成熟。

**SFT 的缺点**

1. **限制模型生成的多样性与泛化能力**：
   - SFT 要求模型尽可能模仿人类提供的特定答案。
   - 在训练中，模型倾向于复制“唯一正确答案”，而不会学习多个有效表达方式，可能导致对同义词或不同表述的惩罚（如困惑度PPL的提高）。
   - 这在任务如写作、总结、翻译中尤其不利，因为这些任务具有天然的多样性。
2. **不直接体现人类偏好**：
   - SFT 是基于静态数据训练的，即便答案由人类提供，但模型并不知道哪些答案更“好”或更符合偏好。
   - 它忽略了不同答案之间的优劣排序，无法学习**偏好导向**的响应策略。
3. **对数据质量和覆盖面的高度依赖**：
   - SFT 效果高度依赖于指令-答案对的质量、丰富性和多样性。
   - 如果训练数据有偏或有限，模型输出容易受到影响，难以泛化到未见场景。

SFT 是将大语言模型与人类意图对齐的重要第一步，但其 **缺乏对人类偏好的直接建模** 和 **对语言表达的多样性支持不足**，限制了其在高质量人机交互中的表现。
 因此，SFT 通常作为后续更先进对齐方法（如 **RLHF，基于人类反馈的强化学习**）的基础步骤。

### RL

强化学习是一种机器学习范式，核心目标是通过智能体（agent）与环境（environment）的互动，**最大化长期累计奖励（cumulative reward）**。

**基本概念：**

- **智能体（Agent）**：学习做决策的主体，例如机器人或语言模型。
- **环境（Environment）**：智能体所处的系统或世界，提供状态和奖励。
- **状态（State）**：当前环境的表征，例如机器人所在的位置。
- **动作（Action）**：智能体在某状态下可以采取的操作。
- **奖励（Reward）**：智能体执行动作后从环境中获得的反馈，用于评估行为好坏。
- **策略（Policy）**：指导智能体在每个状态中采取什么动作的规则或模型。

**强化学习流程（MDP）：**

1. 智能体从初始状态开始；
2. 在每个时间步，智能体依据策略选择一个动作；
3. 动作使环境状态发生变化，环境返回新的状态与奖励；
4. 该过程反复进行，智能体通过试错学习最优策略，以实现最大化累积奖励。

### RL on LLMs

#### 关键映射关系（将RL元素映射到LLM）：

- **Agent（智能体）** → LLM 本身（作为策略）
- **Environment（环境）** → 与人类互动的对话场景
- **State（状态）** → 当前对话上下文或输入 prompt
- **Action（动作）** → 生成下一个 token 或完整响应
- **Reward（奖励）** → 来自人类偏好建模的反馈得分
- **Policy（策略）** → LLM 的 token 生成机制（概率分布）

#### 具体流程：

1. **奖励模型训练（Reward Model, RM）**：
   - 从监督微调后的LLM生成多个响应样本；
   - 由人类对这些响应进行偏好比较，生成“哪一个更好”的对比数据；
   - 用这些偏好数据训练一个奖励模型，用于估计每个响应的质量分数。
2. **基于奖励的策略优化（如PPO）**：
   - 使用奖励模型对LLM输出进行评分；
   - 用强化学习算法（如 PPO，Proximal Policy Optimization）微调原始模型，使其生成更高质量、更符合人类偏好的响应。







# TODO

RealHF --> RLHF

AReaL --> Reasoning RL，需要四个大模型，

From RLHF to Reasoning RL，算法变简单，需要两个大模型，算法变简单。

